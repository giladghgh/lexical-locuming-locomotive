{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Gilad's Thesaurus Engine.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "    // AUTORUN ALL CELLS ON NOTEBOOK-LOAD!\n",
       "    require(\n",
       "        ['base/js/namespace', 'jquery'], \n",
       "        function(jupyter, $) {\n",
       "            $(jupyter.events).on(\"kernel_ready.Kernel\", function () {\n",
       "                console.log(\"Auto-running all cells-below...\");\n",
       "                jupyter.actions.call('jupyter-notebook:run-all-cells-below');\n",
       "                jupyter.actions.call('jupyter-notebook:save-notebook');\n",
       "            });\n",
       "        }\n",
       "    );\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<script>\n",
    "    // AUTORUN ALL CELLS ON NOTEBOOK-LOAD!\n",
    "    require(\n",
    "        ['base/js/namespace', 'jquery'], \n",
    "        function(jupyter, $) {\n",
    "            $(jupyter.events).on(\"kernel_ready.Kernel\", function () {\n",
    "                console.log(\"Auto-running all cells-below...\");\n",
    "                jupyter.actions.call('jupyter-notebook:run-all-cells-below');\n",
    "                jupyter.actions.call('jupyter-notebook:save-notebook');\n",
    "            });\n",
    "        }\n",
    "    );\n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for POS-tagging and synonymision:\n",
    "#! pip install nltk\n",
    "\n",
    "# for lemmatisation and surface realisation:\n",
    "#! pip install lemminflect\n",
    "\n",
    "# other dependencies:\n",
    "#! pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from numpy.random import choice\n",
    "from nltk.corpus import wordnet as wn\n",
    "from lemminflect import getInflection,getLemma\n",
    "\n",
    "Tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apropos punctuation\n",
    "PunctStr = '!#$%&\\'*+-/=/?@\\\\^_`|~'\n",
    "PunctSet = set(PunctStr)\n",
    "\n",
    "DelimStr = ',.;:?!'\n",
    "DelimSet = set(DelimStr)\n",
    "\n",
    "ParenStr = '«»()[]{}<>\"'\n",
    "ParenSet = set(ParenStr)\n",
    "\n",
    "\n",
    "# words with these WordNet tags can be synonymised:\n",
    "# n: noun\n",
    "# v: verb\n",
    "# a: adjective\n",
    "# r: adverb\n",
    "ValidWNTags = [\"n\", \"v\", \"a\", \"r\"]\n",
    "\n",
    "\n",
    "# vowels for reference:\n",
    "Vowels = (\"a\", \"e\", \"i\", \"o\", \"u\")\n",
    "\n",
    "\n",
    "# NLTK labels these immutables wrong, so I must manually clean up.\n",
    "TheCracks = [\"have\", \"has\", \"had\", \"be\", \"is\", \"was\", \"are\", \"been\", \"do\", \"does\", \"did\"]\n",
    "CracksDict = {'have': 'have', 'has': 'have', 'had': 'have', 'be': 'be', 'is': 'be', 'was': 'be', 'are': 'be', 'been': 'be', 'do': 'do', 'does': 'do', 'did': 'do'}\n",
    "\n",
    "\n",
    "# fodder, needs to handle:\n",
    "#  this  + that  ->  thisthat   -   punctuation situation  - space?\n",
    "# \"they\" + \"'ve\" -> \"they've\"   - not only;  starts        -   N\n",
    "#  \"do\"  + \"n't\" ->  \"don't\"    - not only;  middle        -   N\n",
    "#      \"mr.\"     ->   \" mr.\"    - not only;   end          -   Y\n",
    "#     \"x.com\"    ->  \" x.com\"   - not only;   any          -   Y\n",
    "                               #\n",
    "#       \"/\"      ->   \" /\"      -   only;    starts        -   Y\n",
    "#       \"-\"      ->   \" -\"      -   only;    starts        -   Y\n",
    "#       \".\"      ->    \".\"      -   only;    starts        -   N\n",
    "#       \"(\"      ->    \"(\"      -   only;    starts        -   N\n",
    "text0 = \\\n",
    "\"\"\"'One day, the people (that don't even like / love you) are going to tell everyone how they met you' -- Mr. Johnny Depp. What a terrible quote from a hot.com person.\"\"\"\n",
    "\n",
    "text1 = \\\n",
    "\"\"\"word replacement engine\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# private\n",
    "def _translateTag(tag , fmt):\n",
    "    \"\"\" Convert tags from universal tagset to either WordNet or LemmInflect format. \"\"\"\n",
    "    # shouldn't handle MOD or AUX tags, those should bypass this as they are immutable.\n",
    "    otag = str()\n",
    "    if tag.startswith(\"V\"):\n",
    "        otag = \"VERB\" if fmt==\"LI\" else wn.VERB\n",
    "    elif tag.startswith(\"J\"):\n",
    "        otag = \"ADJ\"  if fmt==\"LI\" else wn.ADJ\n",
    "    elif tag.startswith(\"R\"):\n",
    "        otag = \"ADV\"  if fmt==\"LI\" else wn.ADV\n",
    "    elif tag.startswith(\"N\"):\n",
    "        otag = \"NOUN\" if fmt==\"LI\" else wn.NOUN\n",
    "    return otag\n",
    "\n",
    "\n",
    "def _synoname(wordobj):\n",
    "    \"\"\" Return just the synonym (no metadata) from a WordNet synset object. \"\"\"\n",
    "    # surely there's an 'official' way to retrieve the word?\n",
    "    return wordobj.name().split('.')[0] if type(wordobj) is nltk.corpus.reader.wordnet.Synset else wordobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# principal\n",
    "def extract(raw):\n",
    "    \"\"\" Extract information from text at the word level, unaggregated. \"\"\"\n",
    "    # builds [(\"word1\",\"tag1\") , (\"word2\",\"tag2\") , ... ]\n",
    "    wordtags = nltk.pos_tag( nltk.word_tokenize(raw) )\n",
    "    return list(map( lambda wt:(wt[0].lower() , wt[1]) , wordtags ))\n",
    "\n",
    "\n",
    "def lemmatise(wordtags):\n",
    "    \"\"\" Find and filter valid lemmas. \"\"\"\n",
    "    for word,tag in wordtags:\n",
    "        liTag = _translateTag(tag,\"LI\")\n",
    "        if (tag in [\"NNP\"]) or (word in TheCracks) or (len(word) < 3) or not liTag:\n",
    "            lemma = word\n",
    "        else:\n",
    "            lemma = getLemma(word , upos=liTag)[0]\n",
    "            \n",
    "        yield lemma,tag\n",
    "\n",
    "\n",
    "def synonymise(wordtags):\n",
    "    \"\"\" Find and filter valid synonyms\"\"\"\n",
    "    for word,tag in wordtags:\n",
    "        # again, the following get a free pass because NLP is still neotenous:\n",
    "        if (tag in [\"NNP\"]) or (word in TheCracks) or (len(word) < 3):\n",
    "            yield word,tag\n",
    "            continue\n",
    "            \n",
    "        # facilitate discussion between libraries:\n",
    "        wnTag = _translateTag(tag,\"WN\")\n",
    "        \n",
    "        # collect possible synonyms:\n",
    "        synonyms = {lemma for synset in wn.synsets(word,pos=wnTag) for lemma in synset.lemma_names()}\n",
    "        \n",
    "        # POS tags must be mutable (i.e. exclude proper nouns, pronouns, etc.):\n",
    "        synonyms = list(filter( lambda syn:(wnTag in ValidWNTags) , synonyms ))\n",
    "        \n",
    "        # collocations will have an underscore rather than a space, let's fix that:\n",
    "        synonyms = list(map( lambda syn:_synoname(syn).replace(\"_\",\" \") , synonyms ))\n",
    "        \n",
    "        #custom distribution so earlier synonyms are more favourable:\n",
    "        # [1]\n",
    "        # [1/2  1/2]\n",
    "        # [1/2  1/4  1/4]\n",
    "        # [1/2  1/4  1/8  1/8]\n",
    "        # [1/2  1/4  1/8  1/16  1/16]a\\\n",
    "        dist = [2**(-i-1) for i in range(len(synonyms)-1)]\n",
    "        dist.append( 1 - sum(dist) )\n",
    "        \n",
    "        synonym = choice(synonyms , p=dist) if synonyms else word\n",
    "        \n",
    "        yield synonym.lower(),tag\n",
    "\n",
    "\n",
    "def inflectivise(wordtags):\n",
    "    \"\"\" Inflect list of (word,tag) tuples correctly. \"\"\"\n",
    "    wordtaglist = list(wordtags)\n",
    "    for i,(word,tag) in enumerate(wordtaglist):\n",
    "        if (tag in [\"NNP\"]) or (word in TheCracks) or (len(word) < 3):\n",
    "            yield word,tag\n",
    "            continue\n",
    "            \n",
    "        # conjugation and declension:\n",
    "        if word in [\"a\",\"an\"]:\n",
    "            inflected = \"a\"\n",
    "            # ideally I'd transcribe to IPA and check for vowel phonemes, but this'll do for now.\n",
    "            if wordtaglist[i+1][0].startswith(Vowels):\n",
    "                inflected = \"an\"\n",
    "        elif \" \" not in word:\n",
    "            inflected = getInflection(word,tag=tag)\n",
    "            inflected = word if not inflected else inflected[-1]\n",
    "        else:\n",
    "            # only conjugate the relevant part of a collocation:\n",
    "            # e.g. \"run across\"  ->  \"ran across\"  NOT  \"run acrossed\"\n",
    "            for subword,subtag in nltk.pos_tag( nltk.word_tokenize(word) ):\n",
    "                if _translateTag(subtag,\"WN\") in ValidWNTags:\n",
    "                    infl = getInflection(subword,tag=tag)[-1]\n",
    "                    inflected = word.replace(subword , infl)\n",
    "                \n",
    "        yield inflected,tag\n",
    "\n",
    "\n",
    "def assemble(wordtags):\n",
    "    \"\"\" Autobots, assemble! \"\"\"\n",
    "    first = True\n",
    "    for word,tag in wordtags:\n",
    "        # truecasing proper nouns and \"I\":\n",
    "        word = word.title() if (tag in [\"NNP\",\"NNPS\"]) or (word == \"i\") else word\n",
    "        \n",
    "        # conditions in case of contractions or punctuation:\n",
    "        wordset = set(word)\n",
    "        hasPunct = bool( wordset & PunctSet )\n",
    "        \n",
    "        # without apostrophe, contractions have no punctuation:\n",
    "        isContraction = hasPunct and not bool( (wordset - {\"'\"}) & PunctSet )\n",
    "        isClauseDelim = not bool( wordset - DelimSet )\n",
    "        isParentheses = not bool( wordset - ParenSet )\n",
    "        \n",
    "        if first:\n",
    "            first = False\n",
    "            draft = word\n",
    "        elif isContraction or isClauseDelim:\n",
    "            draft += word\n",
    "        else:\n",
    "            draft += \" \" + word\n",
    "    \n",
    "    # truecasing sentences:\n",
    "    sentences = list()\n",
    "    for sentence in Tokenizer.tokenize(draft):\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        sentences.append(sentence.replace( words[0] , words[0].title() , 1 ))\n",
    "    final = ' '.join(sentences)\n",
    "    \n",
    "    # spacing surrounding parenthesese:\n",
    "    final = re.sub( r'''(?<=[{[(])\\s+     # ((any single opening bracket))  preceding  (at least one whitespace)\n",
    "                        |                 # or\n",
    "                        \\s+(?=[]})])      # (at least one whitespace)  preceding  ((any single closing bracket))\n",
    "                        ''', '' , final , flags=re.VERBOSE)\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Better to remain silent and be thought a fool than to speak and remove all doubt' -- Mark Twain. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid upos type = None\n",
      "Invalid upos type = None\n",
      "Invalid upos type = None\n",
      "Invalid upos type = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Better to stay on still and be considered a mark than to speak and slay all doubtfulness' -- Mark Twain.\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt','r') as file:\n",
    "    text = file.read()\n",
    "    #text = text0\n",
    "print(text, \"\\n\")\n",
    "    \n",
    "parsedText = extract(text)\n",
    "    \n",
    "lemmaText = lemmatise(parsedText)\n",
    "    \n",
    "synonymText = synonymise(lemmaText)\n",
    "    \n",
    "inflectedText = inflectivise(synonymText)\n",
    "\n",
    "print(assemble(inflectedText))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
